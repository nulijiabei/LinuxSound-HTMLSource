<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title> Linux Sound Architecture</title>

    <link rel="stylesheet" type="text/css" href="../../stylesheet.css" />

    <style type="text/css">
      body { counter-reset: chapter 4; }
    </style>

    <script type="text/javascript" src="../../toc.js"> 
      // empty
    </script>

    <script type="text/javascript">
      /* <![CDATA[ */
    (function() {
        var s = document.createElement("script"), t = document.getElementsByTagName("script")[0];
        s.type = "text/javascript";
        s.async = true;
        s.src = "http://api.flattr.com/js/0.6/load.js?mode=auto";
        t.parentNode.insertBefore(s, t);
    })();
/* ]]> */
    </script>

  </head>
  <body>

    <!--#include virtual="../../header.html" -->


    <div class="chapter">
      <h1> Overview of Linux sound architecture </h1>
    </div>

    <div class="preface">   
      <p>
	The Linux sound system, like most of Linux, has evolved from a simple system
	to a much more complex one. This chapter gives a high level overview
	of the components of the Linux sound system, and which bits are best
	used for which use cases.
      </p>
    </div>

    <div id="generated-toc" class="generate_from_h2"></div>

    <h2> Resources </h2>
    <ul>
      <li>
      Lennart Poettering has 
      <a href="http://0pointer.de/blog/projects/guide-to-sound-apis.html">
	A Guide Through The Linux Sound API Jungle
      </a>
      </li>

      <li>
      Also, by TuxRadar:
      <a href="http://tuxradar.com/content/how-it-works-linux-audio-explained">
	How it works: Linux audio explained
      </a>
      </li>

      <li>
      Insane Coder posted an article in favour of OSSv4
      <a href="http://insanecoding.blogspot.com.au/2009/06/state-of-sound-in-linux-not-so-sorry.html">
	State of sound in Linux not so sorry after all
      </a>
      which drew a lot of comments...
      </li>
    </ul>

    <h2> Components </h2>
    <p>
      The following diagram indicates the different layers:
      <br/>
      <img alt="layers of sound system" src="layers.png"/>
    </p>

    <h3> Device drivers </h3>
    <p>
      At the bottom layer is the hardware itself, the audio device. These are the audio cards
      made by a variety of manufacturers, all with different capabilities, interfaces and prices.
      Just like any piece of hardware, in order for it to be visible and useful to the Operating
      System there must be a device driver. There are of courses thousands of device drivers
      written for Linux. Writing Linux device drivers is a speciality of itself, and there
      are dedicated sources for this, such as
      <a href="http://lwn.net/Kernel/LDD3/">
	Linux Device Drivers, Third Edition
      </a> by Jonathan Corbet, Alessandro Rubini, and Greg Kroah-Hartman.
    </p>

    <p>
      Device drivers must have standardised APIs "at the top" so that users of the device have a
      known interface to code to. The OSS device driver API was used for audio devices until
      it was made closed source, at which point developers switched to the ALSA API.
      While OSS v4 has become open again, the ALSA interface is supported in the kernel
      while OSS is not.
    </p>

    <p>
      Ideally, a device driver API should expose all of the features of hardware while not
      adding additional baggage. For audio, it is not always so easy to set boundaries for
      what an audio driver should do. For example, some sound cards will support mixing
      of analog signals from different sources, while others will not, and some sound cards
      will have MIDI synthesizers while others will not. If the API is to expose these
      capabilities for sound cards which support them, then it might have to supply them in software
      for those sound cards which do not.
    </p>

    <p>
      There is a somewhat dated document on 
      <a href="http://www.alsa-project.org/~tiwai/writing-an-alsa-driver/">
	writing ALSA device drivers
      </a> by Takashi Iwai
    </p>

    <h3> Sound servers </h3>
    <p>
      Linux is a multi-tasking, multi-threaded operating system. It is possible that concurrent
      processes might wish to write sounds to the audio cards concurrently. For example,
      a mail reader might wish to "ding" the user to report new mail, 
      even if they are in the middle of a noisy
      computer game. This is distinct from sound card capabilities of being able to mix sounds
      from different ports, such as an HDMI input port and an analog input port. It requires the ability
      to mix (or otherwise manage) sounds from different processes. As an example of the subtlety
      of this, should the volume of each process be individually controllable, or should the
      destination port (headphones or speaker) be individually controllable?
    </p>

    <p>
      Such capabilities are beyond the scope of a device driver. Linux resolves this by having
      "sound servers" which run above the device drivers and manage these more complex tasks.
      Above these will sit applications which talk to the sound server which in turn will
      pass the resultant digital signal to the device driver.
    </p>

    <p>
      Here is where a significant difference occurs between sound servers. For professional audio
      systems the sound server must be able to process and route audio with the minimal amount
      of latency or other negative effects. For consumer audio, control over volumes and destinations
      may be more important than altency - you probably won't care if a new message "ding" takes an extra
      half-second. Between these may be other cases such as games requiring synchronisation of 
      audio and visual effects, and Karaoke players requiring synchronisation of analog and 
      digital sources.
    </p>

    <p>
      The two major sound servers under Linux are Jack for professional audio and PulseAudio
      for consumer systems. They are designed for different use-cases and consequently offer
      different features.
    </p>

    <p>
      Lennart Poettering in
      <a href="http://0pointer.de/blog/projects/guide-to-sound-apis.html">
	A Guide Through The Linux Sound API Jungle
      </a>
      offers a good summary of these different use cases:
    </p>
      <blockquote>
	<dl>
	  <dt>
	    I want to write a media-player-like application!
	  </dt>
	  <dd>
	    Use GStreamer! (Unless your focus is only KDE in which cases Phonon might be an alternative.)
	  </dd>
	  <dt>
	    I want to add event sounds to my application!
	  </dt>
	  <dd>
	    Use libcanberra, install your sound files according to the XDG Sound Theming/Naming Specifications! (Unless your focus is only KDE in which case KNotify might be an alternative although it has a different focus.)
	  </dd>
	  <dt>
	    I want to do professional audio programming, hard-disk recording, music synthesizing, MIDI interfacing!
	  </dt>
	  <dd>
	    Use JACK and/or the full ALSA interface.
	  </dd>
	  <dt>
	    I want to do basic PCM audio playback/capturing!
	  </dt>
	  <dd>
	    Use the safe ALSA subset.
	  </dd>
	  <dt>
	    I want to add sound to my game!
	  </dt>
	  <dd>
	    Use the audio API of SDL for full-screen games, libcanberra for simple games with standard UIs such as Gtk+.
	  </dd>
	  <dt>
	    I want to write a mixer application!
	  </dt>
	  <dd>
	    Use the layer you want to support directly: if you want to support enhanced desktop software mixers, use the PulseAudio volume control APIs. If you want to support hardware mixers, use the ALSA mixer APIs.
	  </dd>
	  <dt>
	    I want to write audio software for the plumbing layer!
	  </dt>
	  <dd>
	    Use the full ALSA stack.
	  </dd>
	  <dt>
	    I want to write audio software for embedded applications!
	  </dt>
	  <dd>
	    For technical appliances usually the safe ALSA subset is a good choice, this however depends highly on your use-case. 
	  </dd>
	</dl>
      </blockquote>

    <h3> Complexities </h3>
    <p>
      The simple diagram I drew above hides the real complexities of Linux sound.
      Mike Melanson (an Adobe engineer) in 2007 produced this diagram
      <br/>
      <img alt="Linux sound dependencies" src="linuxaudio.png"/>
    </p>

    <p>
      The figure is not up-to-date: for example,
      OSS is no longer a major part of Linux.
      Some special-case complexities are, for example,
      that  PulseAudio sits above ALSA,
      and it also sits <em>below</em>
      ALSA as in this figure from 
      <a href="http://insanecoding.blogspot.com.au/2009/06/state-of-sound-in-linux-not-so-sorry.html">
	insane coding
      </a>: <br/>
      <img alt="Sound layers" src="http://2.bp.blogspot.com/_vLES3KKBdaM/SjsQ-L2UVII/AAAAAAAAAFs/Vcm87Z3KMDw/s320/alsalib.png">
      <br/> (This diagram is upside-down compared to mine.)
    </p>

    <p>
      The explanation is as follows:
    </p>
      <ul>
	<li>
	  PulseAudio is able to do things like mixing application sounds 
	  that ALSA cannot do
	</li>
	<li>
	  PulseAudio installs itself as the default ALSA output device
	</li>
	<li>
	  An application sends audio to the ALSA default device which sends
	  it to PulseAudio
	</li>
	<li>
	  PulseAudio mixes it it with any other audio and then sends it back
	  to a particular device in ALSA
	</li>
	<li>
	  ALSA then plays the mixed sound
	</li>
      </ul>
<p> 
     Complex, yes, but is accomplishes tasks that wouild be difficult otherwise
.
</p>
 
      <h2> Conclusion </h2>
    <p>
      The architecture of Linux sound systems is complex, and new
      wrinkles are being added on a regular basis. However, this is the
      same for any audio system. Successive chapters will flesh
      out the detals of many of these components.
    </p>


     <hr/>
 <!--#include virtual="../../footer.html" -->

  </body>
</html>
